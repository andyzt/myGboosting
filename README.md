# myGboosting
Простая реализация градиентного бустинга. Работает с задачами регрессии, использует метрику MSE.

## Использование

Утилита предполагает вызов из командной строки. 
В качестве обучающей и тестовой выборки используются стандартные файлы csv. 
Утилита имеет 2 режима работы: обучение (fit) и применение (predict).

### Сборка проекта
перейти в каталог src, создать в нем каталог build, перейти в него и выполнить следующие команды:
    
    cmake ..
    make -j4

при этом на компьютере должны быть установлены библиотеки Open MP и protobuf. 

### Обучение

Формат запуска:

`myGboosting fit <file path> [optional parameters]`

`<file path>` - путь к csv файлу, содержащему обучающую выборку
#### Параметры:

| Параметр       | Описание                                                                            | Значение по умолчанию |
|----------------|-------------------------------------------------------------------------------------|-----------------------|
| column_names   | путь к файлу, содержащему названия столбцов обучающей выборки                       |                       |
| model-path     | имя файла, в который будет сохранена обученная модель                               |                       |
| output-path    | имя файла, в который будут сохранены прогнозы обученной модели на обучающей выборке |                       |
| target         | номер колонки, которая содержит значение целевой переменной                         | последняя колонка(-1) |
| nthread        | число параллельных потоков, используемых для обучения                               |          1            |
| delimiter      | разделитель, используемый в csv-файлах                                              |          ,            |
| has-header     | имеет ли входной csv-файл заголовок с названиями столбцов                           |        false          |
| iterations     | максимальное количество деревьев в модели                                           |        100            |
| learning-rate  | темп обучения модели                                                                |        1.0            |
| depth          | глубина решающего дерева                                                            |        6              |
| max_bins       | количество сплитов в гистограмме для числовых признаков (от 1 до 255)               |        10             |
| verbose        | степень подробности выводимой в консоль информации                                  |        0              |
| sample_rate    | вероятность сэмплинга строк для каждого дерева (какую часть датасета использовать)  |        0.66           |
| min_leaf_count | минимальное количество объектов в листовой вершине                                  |        1              |


### Применение

Формат запуска:

myGboosting predict \<file path> [optional parameters]

\<file path> - путь к csv файлу, содержащему тестовую выборку
#### Параметры:

| Параметр       | Описание                                                                            | Значение по умолчанию |
|----------------|-------------------------------------------------------------------------------------|-----------------------|
| column_names   | путь к файлу, содержащему названия столбцов тестовой выборки                        |                       |
| model-path     | имя файла, из которого будет считана обученная модель                               |                       |
| output-path    | имя файла, в который будут сохранены прогнозы модели на тестовой выборке            |                       |
| delimiter      | разделитель, используемый в csv-файлах                                              |           ,           |
| has-header     | имеет ли входной csv-файл заголовок с названиями столбцов                           |         false         |
| has-target     | имеет ли входной csv-файл колонку со значениями целевой переменной                  |         false         |
| target         | номер колонки, которая содержит значение целевой переменной                         | последняя колонка(-1) |
| verbose        | степень подробности выводимой в консоль информации                                  |           0           |


### Архитектура 

- Используются Oblivious Decision trees и гистограммы признаков
- Параллелизация при обучении происходит при выборе оптимального сплита c помощью библиотеки Open MP
- для сохранения и загрузки моделей используется библиотека Protobuf

### Результаты

Модель проверялась на наборе данных Higgs, была взята train выборка (250000 сэмплов) 
и поделена на train (20000) и тест (50000). Поскольку последняя колонка - 
это бинарная классификация, то предсказывалась предпоследняя (вес частицы).

#### Параметры запуска:
LightGBM

    time ./lightgbm objective=mse data=../../myGboosting/testing/datasets/Higgs/train.csv 
    num_threads=1 num_iterations=400 max_bin=255 bagging_fraction=0.5 feature_fraction=1.0 bagging_freq=1 
    num_leaves=64 learning_rate=0.5 label=31 min_data_in_leaf=1

myGboosting

    time ./myGboosting fit  ../testing/datasets/Higgs/train.csv --output=model.pb 
    --iterations=400 --depth=6 --learning-rate=0.5 --sample-rate=0.5 --max_bins=255 --nthreads=1

#### Результаты на 1 потоке
Модель проверялась на ноутбуке Macbook Pro 15 2015. Поэтому в наличии есть 4 реальных ядра и 8 виртуальных.

| Решение   | Depth | Row sampling | Кол-во деревьев | Learning Rate |Время   | MSE Train | MSE Test      | 
|-----------|-------|--------------|-----------------|---------------|--------|-----------|---------------|
| LightGBM  |   6   |     0.5      |       400       |      0.5      |13.352s | 0.6469    | 1.88662       |
|myGboosting|   6   |     0.5      |       400       |      0.5      |12.062s | 1.11156   | 1.44719       |
| LightGBM  |   6   |     0.7      |       400       |      0.5      |13.643s | 0.5028    |               |
|myGboosting|   6   |     0.7      |       400       |      0.5      |13.974s | 1.06907   |               |
| LightGBM  |   6   |     1.0      |       400       |      0.5      |13.483s | 0.4661    |               |
|myGboosting|   6   |     1.0      |       400       |      0.5      |16.315s | 1.05107   |               |

Странным образом на LightGBM влияет параметр Row sampling. При его уменьшении скорость должна расти, а у него она падает.

#### Результаты в многопоточном режиме (время)

| Решение   | 1 поток |2 потока |4 потока |6 потоков|8 потоков|
|-----------|---------|---------|---------|---------|-------- |
| LightGBM  | 13.352s | 10.404s | 8.727s  | 9.070s  | 9.766s  |      
|myGboosting| 12.062s | 8.915s  | 7.604s  | 7.304s  | 7.313s  |

Видно, что после 4 потоков производительность почти не растет.


### Ход наших экспериментов

1)	Мы реализовали базовую версию решающего дерева, которая перебирала все возможные сплиты.
2)	Стало понятно, что это работает очень медленно, и мы перешли к гистограммам
3)	Далее, возникла идея перенести гистограммы на уровень фичей и добавить работу с категориальными признаками. Для этого мы реализовали бинаризацию численных признаков и one hot кодирование для категориальных признаков
4)	В ходе замеров производительности выяснилось, что данный подход работает медленно и требует большого количества памяти на больших датасетах (Higgs).
5)	 После этого мы перешли к реализации Oblivious Decision Trees (ODT). Мы выбрали вариант ODT, при котором мы выбираем одно значение сплита для всех узлов конкретного уровня, это позволяет хранить дерево как список сплитов (id_признака, № корзины) и список значений в листовых вершинах (2^<Глубина дерева> вершин)
6)	Эксперименты с ODT показали, что MSE уменьшается гораздо медленнее, чем на обычных деревьях. После отладки выяснилось, что наше разбиение на корзины работает не оптимально, и мы взяли его реализацию из LightGBM.
7)	Кроме того, на MSE негативно влиял параметр min_child_weight (минимальное кол-во объектов в листах). Выяснилось, что для ODT оптимальнее строить полные бинарные деревья, независимо от числа объектов в листьях (min_child_weight=1).
8)	Теперь MSE стало вести себя гораздо лучше.
9)	Также одной из особенностей ODT является то, что фичи в одном и том же дереве не повторяются. Мы пробовали отключать это правило и на небольших датасетах это давало лучшее уменьшение MSE, но на больших датасетах MSE уменьшается одинаково, а с включенным правилом алгоритм работает быстрее.
10)	Далее мы перешли к реализации нескольких потоков. После профилирования с помощью Valgrind выяснилось, что основное время работы алгоритма занимает построение гистограмм. Поэтому для приемлемой производительности многопоточного алгоритма оказалось достаточно распараллелить поиск оптимального сплита, в рамках которого у нас строятся гистограммы.
11)	Для реализации параллельных потоков была выбрана библиотека OpenMP, т.к. LightGBM и XGBoost используют ее же. Параллельность была реализована с помощью параллельного цикла for с критической секцией по выбору максимального Gain.
12)	При сравнении производительности с LightGBM была замечена странная особенность: изменение параметра row sampling (сэмплирование датасета для конкретного дерева) не приводит к увеличению производительности LightGBM. При этом наше решение увеличивает свою производительность (что логично, т.к. дереву нужно учиться на меньшем объеме данных).


### Используемые библиотеки

1) https://github.com/ben-strasser/fast-cpp-csv-parser
2) https://github.com/Taywee/args
3) https://developers.google.com/protocol-buffers/
4) https://www.openmp.org

